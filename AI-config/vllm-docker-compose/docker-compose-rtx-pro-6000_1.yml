services:
  vllm:
    image: vllm/vllm-openai:v0.12.0
    container_name: vllm-qwen3next-80b
    ports:
      - "8123:8000"
    volumes:
      - "D:/dev_software/AI_models/huggingface/Qwen3-Next-80B-A3B-Instruct-FP8:/models/Qwen3-Next-80B-A3B-Instruct-FP8:ro"
      # # 可选：挂 HuggingFace cache（避免容器内重复下载 tokenizer/依赖）
      # - "D:/dev_software/AI_models/huggingface/_cache:/root/.cache/huggingface"
    gpus: all
    ipc: host
    shm_size: "32gb"

    environment:
      VLLM_USE_FLASHINFER_MOE_FP8: "1"
      VLLM_FLASHINFER_MOE_BACKEND: "latency"
      VLLM_USE_DEEP_GEMM: "0"
      VLLM_USE_TRTLLM_ATTENTION: "0"
      VLLM_ATTENTION_BACKEND: "FLASH_ATTN"

    command:
      - "/models/Qwen3-Next-80B-A3B-Instruct-FP8"
      - "--served-model-name"
      - "Qwen3-Next-80B-A3B-Instruct-FP8"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--override-generation-config"
      - '{"temperature":0.2,"top_p":0.9}'

      # 工具调用
      - "--enable-auto-tool-choice"
      - "--tool-call-parser"
      - "hermes"

      # 上下文 + 并发设置
      - "--max-model-len"
      - "8192"
      - "--max-num-seqs"
      - "8"
      - "--max-num-batched-tokens"
      - "16384"
      - "--gpu-memory-utilization"
      - "0.90"
      - "--enable-prefix-caching"
      - "--enable-chunked-prefill"

      # # KV cache 用 FP8，省显存，提升并发承载, 可能有bug
      # - "--kv-cache-dtype"
      # - "fp8_e5m2"
      # - "--calculate-kv-scales"