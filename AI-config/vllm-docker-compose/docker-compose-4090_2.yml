services:
  vllm:
    image: vllm/vllm-openai:v0.12.0
    container_name: vllm-qwen3-4090_2
    ports:
      - "8000:8000"
    volumes:
      - "D:/dev_software/AI_models/huggingface/Qwen3-30B-A3B-FP8:/models/Qwen3-30B-A3B-FP8:ro"
      # # 可选：挂 HuggingFace cache（避免容器内重复下载 tokenizer/依赖）
      # - "D:/dev_software/AI_models/huggingface/_cache:/root/.cache/huggingface"
    gpus: "device=0,1"
    ipc: host
    shm_size: "24gb"

    command:
      - "--model"
      - "/models/Qwen3-30B-A3B-FP8"
      - "--served-model-name"
      - "Qwen3-30B-A3B-FP8"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"

      # 两卡张量并行
      - "--tensor-parallel-size"
      - "2"

      # 工具调用
      - "--enable-auto-tool-choice"
      - "--tool-call-parser"
      - "hermes"

      # 并发/吞吐给更宽松的上限
      - "--max-model-len"
      - "8192"
      - "--max-num-seqs"
      - "12"
      - "--max-num-batched-tokens"
      - "16384"
      - "--enable-prefix-caching"
      - "--enable-chunked-prefill"
      - "--gpu-memory-utilization"
      - "0.90"

      # KV cache 用 FP8，省显存，提升并发承载
      - "--kv-cache-dtype"
      - "fp8_e5m2"
      - "--calculate-kv-scales"