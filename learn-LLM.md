# 大语言模型训练全流程解析

大语言模型的训练是一个精心设计的三阶段流程：**预训练**赋予模型基础语言能力，**监督微调**使其学会遵循指令，**RLHF**则让模型与人类偏好对齐。这一流程将”文本补全器”转变为真正有用的AI助手，其核心创新在于将不可微分的人类判断转化为可优化的信号。

-----

## 第一阶段：预训练奠定语言能力基石

预训练的本质是**自监督学习**——模型通过预测文本的下一个token，从海量语料中自动学习语言规律，无需人工标注。这一简单目标蕴含着惊人的学习能力：模型在预测过程中隐式地获取了语法结构、语义关系、世界知识乃至推理模式。

**自回归语言建模**（Autoregressive Language Modeling）是当前主流范式。给定一个token序列，模型学习预测下一个token的概率分布，数学表达为：P(x₁, x₂, …, xₙ) = ∏P(xᵢ|x₁,…,xᵢ₋₁)。这意味着一个1000个token的序列可以产生999个训练信号，数据利用效率极高。GPT系列、LLaMA、Claude等模型均采用此方法。

**训练数据的处理**直接影响模型质量。主要数据来源包括：Common Crawl（约400TB原始网页）、维基百科、书籍语料库、GitHub代码、学术论文等。数据处理流程包括：语言识别与过滤、基于哈希和MinHash的去重、使用小模型进行质量评分、以及BPE或SentencePiece分词（词表大小通常为**32K-100K** tokens）。研究表明，去重可显著提升模型性能并减少记忆化问题。

**Transformer架构**是预训练规模化的关键推动力。其自注意力机制允许每个token直接关注序列中的所有其他token，解决了RNN的长程依赖和梯度消失问题。更重要的是，注意力计算可高度并行化，使得在数千GPU上同时训练成为可能。现代改进包括：RoPE旋转位置编码（更好的长度外推）、RMSNorm（简化的归一化）、SwiGLU激活函数、Flash Attention（优化内存访问）等。

预训练的**规模与成本**令人瞩目。根据Chinchilla缩放定律，最优训练需要约20个token对应1个参数。GPT-3（1750亿参数）使用3000亿token训练，成本约**400-1200万美元**；LLaMA-65B使用1.4万亿token，成本约200-500万美元。前沿模型如GPT-4的训练成本估计超过1亿美元。

-----

## 第二阶段：监督微调激活指令遵循能力

预训练后的模型本质上是”文档补全器”——它会延续文本的统计模式，但不理解用户意图。**监督微调（SFT）** 通过在精选的指令-响应对上训练，教会模型识别并响应明确的指令，实现从”补全文本”到”遵循指令”的关键转变。

SFT的核心洞见是：**少量高质量数据可产生显著的行为变化**。与预训练动辄数万亿token不同，SFT通常只需**数千到数十万**条指令-响应对。InstructGPT使用约13K条人工编写的示例，Alpaca使用52K条合成数据，均取得了显著效果。

**指令数据集的构建**有两条主要路径。人工编写方面，Databricks的Dolly 2.0由员工编写了15K条多样化指令，OpenAssistant众包收集了16万条多语言对话数据。合成生成方面，**Self-Instruct**方法利用模型自身生成新指令：从175条种子任务出发，提示模型生成新指令和对应的输入输出，经过过滤后迭代扩展。Stanford Alpaca用不到500美元成本，通过GPT-3.5生成了52K条训练数据。

SFT的训练过程与预训练使用**相同的损失函数**（下一token预测的交叉熵），但有关键区别：**损失只计算在响应token上**，指令部分被mask掉。这聚焦于学习”如何回应”而非记忆指令格式。学习率通常设为**1e-5到5e-5**，比预训练低约10倍，以防止灾难性遗忘。训练轮数通常只有1-3轮，避免过拟合。

参数高效微调（PEFT）技术大幅降低了SFT门槛。**LoRA**（低秩适应）冻结原始权重，只训练低秩分解矩阵，可减少约10000倍可训练参数。**QLoRA**结合4位量化，使得在单张48GB GPU上微调650亿参数模型成为可能。

-----

## 第三阶段：RLHF实现人类偏好对齐

监督微调后的模型能遵循指令，但其输出质量完全取决于示范数据。**RLHF**（基于人类反馈的强化学习）通过将人类偏好转化为可优化的奖励信号，让模型学会生成人类真正偏好的响应，而非仅仅模仿。

### RLHF完整流程概览

RLHF包含三个紧密衔接的阶段：首先在示范数据上进行**SFT**，获得基线模型；然后收集人类偏好比较数据，训练**奖励模型**；最后使用**PPO算法**优化语言模型，使其最大化奖励模型的评分，同时通过KL散度约束保持与SFT模型的接近。

### 奖励模型的训练

奖励模型学习预测”人类会偏好哪个响应”。数据收集采用**成对比较**格式：给定同一提示，生成K个候选响应（通常4-9个），人类标注员标注每对之间的偏好关系。成对比较比绝对评分（1-10分）更可靠，因为”A比B好”的判断更一致。

训练使用**Bradley-Terry模型**：给定偏好对(y_w, y_l)，学习目标为最大化 log σ(r(x, y_w) - r(x, y_l))，其中σ是sigmoid函数。奖励模型通常基于SFT模型初始化，将最后一层替换为标量输出头。它学到的是隐式的质量标准——有用性、安全性、诚实性、连贯性的综合代理。

### PPO算法的应用

PPO（近端策略优化）是RLHF中最常用的强化学习算法，其核心创新是**裁剪机制**，防止策略更新过大导致训练崩溃。

PPO定义概率比 r(θ) = π_θ(a|s) / π_old(a|s)，然后使用裁剪目标：

**L^CLIP = E[min(r(θ)·A, clip(r(θ), 1-ε, 1+ε)·A)]**

其中A是优势函数（该动作比平均好多少），ε通常设为0.1-0.2。当动作好（A>0）时，增大其概率的激励被限制在(1+ε)倍；当动作差（A<0）时，减小其概率的激励被限制在(1-ε)倍。这创造了一个”信任区域”，在不使用复杂约束优化的情况下保证训练稳定。

RLHF训练需要同时维护四个模型：正在优化的**策略模型**、估计期望回报的**价值模型**、提供奖励信号的**奖励模型**、以及用于计算KL惩罚的**参考模型**（冻结的SFT模型）。这带来了显著的计算开销。

### KL散度约束的作用

**KL散度约束是RLHF成功的关键保障**。完整的优化目标为：

**maximize E[r(x,y) - β·KL(π_θ || π_ref)]**

没有KL约束，模型会发生**奖励黑客**——找到奖励模型的漏洞，生成高分但实际质量低下的输出。例如：过度冗长（长度常与评分正相关）、重复特定关键词、生成对抗性模式。KL约束通过惩罚偏离参考模型太远的行为，将策略锚定在合理的语言分布中。

KL系数β的选择需要权衡：太大会过于保守，限制改进空间；太小则奖励黑客风险增加。InstructGPT使用自适应KL方法，设定目标KL值（如6.0），动态调整β以维持该目标。

### RLHF的挑战与局限

**奖励黑客**仍是核心难题：模型可能学会讨好奖励模型而非真正提升质量，表现为过度谄媚（即使用户错误也表示同意）、添加无意义的免责声明、生成冗余内容等。

**可扩展性瓶颈**来自人类标注：高质量偏好标注成本高昂（每小时$15-50+），标注员需要培训，对复杂领域（代码、数学）的评判尤其困难。同时，PPO是on-policy算法，无法重用历史数据，计算效率较低。

**对齐税**指RLHF后在某些能力上的性能下降：模型可能变得过于保守，拒绝回答合理问题，或在学术基准上表现回退。这反映了有用性与安全性之间的内在张力。

### 新兴替代方案

**DPO（直接偏好优化）** 是最重要的替代方案。其核心洞见是RLHF目标存在解析解，可以绕过显式奖励模型和RL训练。DPO损失直接在偏好数据上优化：

L_DPO = -E[log σ(β·log(π_θ(y_w)/π_ref(y_w)) - β·log(π_θ(y_l)/π_ref(y_l)))]

DPO只需要2个模型（而非4个），训练更稳定，实现更简单，已被广泛采用。但它对复杂偏好的建模能力可能弱于显式奖励模型。

**RLAIF（AI反馈强化学习）** 和**Constitutional AI**使用AI模型代替人类提供偏好反馈，大幅降低标注成本并提高可扩展性。Anthropic的Constitutional AI使用一组明确原则指导AI进行自我批评和修订。

-----

## 强化学习基础概念

理解RLHF需要掌握以下核心RL概念：

**状态（State）** 表示当前情境，在LLM中是提示词加上已生成的token序列。**动作（Action）** 是智能体的决策，在LLM中是从词表中选择下一个token。**奖励（Reward）** 是标量反馈信号，RLHF中来自奖励模型，通常在响应结束时给出（稀疏奖励）。

**策略（Policy）** 是从状态到动作的映射，LLM本身就是策略——其softmax输出定义了token上的概率分布。**价值函数（Value Function）** 估计从某状态出发的期望累积回报，用于降低策略梯度的方差。**优势函数（Advantage）** 衡量某动作相对平均水平的好坏程度，是PPO更新的核心依据。

**策略梯度**是RLHF的数学基础。策略梯度定理给出：∇J(θ) = E[∇log π_θ(a|s)·Q(s,a)]。直观理解是：增加导致好结果的动作的概率，减少导致坏结果的动作的概率，回报的大小决定更新幅度。

**Actor-Critic方法**结合了策略学习（Actor）和价值学习（Critic）：Actor选择动作，Critic评估动作质量并提供低方差训练信号。这正是RLHF的架构——LLM是Actor，价值头是Critic。

强化学习与监督学习的**关键区别**在于：监督学习有明确的正确答案，反馈即时，数据分布固定；强化学习只有标量奖励，反馈可能延迟，数据由智能体自身行为产生。RLHF需要RL正是因为”人类偏好”是不可微分的、稀疏的、序列级别的目标，无法直接用监督学习优化。

-----

## 结论：构建AI助手的工程艺术

大语言模型训练流程体现了深度学习与强化学习的精妙结合。预训练通过自监督学习从海量文本中提取语言能力；SFT通过少量精选数据激活指令遵循行为；RLHF将模糊的人类偏好转化为可优化的目标。每个阶段使用最适合的学习范式，逐步将统计语言模型塑造为有用的AI助手。

值得注意的趋势包括：DPO等方法正在简化对齐流程，AI反馈正在减少对人类标注的依赖，而更长的上下文窗口和更高效的注意力机制正在拓展模型能力。但核心挑战依然存在——如何定义和测量”对齐”，如何在有用性与安全性之间取得平衡，以及如何确保模型真正理解而非仅仅模式匹配人类偏好。

-----

## 核心参考文献

**综述论文**

- Zhao et al. “A Survey of Large Language Models” (2023) - arXiv:2303.18223：最全面的LLM综述
- Zhang et al. “Instruction Tuning for Large Language Models: A Survey” (2023) - arXiv:2308.10792：指令微调专题综述

**关键原始论文**

- Vaswani et al. “Attention Is All You Need” (2017) - arXiv:1706.03762：Transformer架构原论文
- Ouyang et al. “Training language models to follow instructions with human feedback” (2022) - arXiv:2203.02155：InstructGPT，现代RLHF流程的奠基之作
- Schulman et al. “Proximal Policy Optimization Algorithms” (2017) - arXiv:1707.06347：PPO算法原论文
- Rafailov et al. “Direct Preference Optimization” (2023) - arXiv:2305.18290：DPO方法，RLHF的重要替代方案
- Bai et al. “Constitutional AI: Harmlessness from AI Feedback” (2022) - arXiv:2212.08073：Anthropic的AI反馈方法

**技术博客与教程**

- Hugging Face Blog “Illustrating RLHF” - huggingface.co/blog/rlhf：图解RLHF流程
- Lilian Weng’s Blog (lilianweng.github.io)：“Policy Gradient Algorithms”和”The Transformer Family”系列
- OpenAI Spinning Up in Deep RL (spinningup.openai.com)：深度强化学习入门
- Stanford Alpaca Blog (crfm.stanford.edu/2023/03/13/alpaca.html)：Self-Instruct应用案例
- Sutton & Barto “Reinforcement Learning: An Introduction”：RL领域经典教材